{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1. Identify Categorical (Qualitative) Variables (CE1)**:\n",
    "Based on the provided data, the following seem to be categorical variables:\n",
    "- BuildingType\n",
    "- PrimaryPropertyType\n",
    "- PropertyName\n",
    "- Address\n",
    "- City\n",
    "- State\n",
    "- ZipCode\n",
    "- TaxParcelIdentificationNumber\n",
    "- CouncilDistrictCode\n",
    "- Neighborhood\n",
    "- DefaultData\n",
    "- ComplianceStatus\n",
    "- Outlier\n",
    "\n",
    "### **2. Transform Categorical Variables (CE2)**:\n",
    "We can use `OneHotEncoder` for most of the categorical variables, but for those with a high cardinality (like `PropertyName` or `Address`), it might be better to use `TargetEncoder` or simply drop them based on the specific use-case.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Assuming we have a dataframe called df\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "target_enc = TargetEncoder()\n",
    "\n",
    "# OneHotEncode low cardinality features\n",
    "low_cardinality_features = ['BuildingType', 'City', 'State', 'CouncilDistrictCode', 'ComplianceStatus', 'Outlier']\n",
    "encoded_low_cardinality = ohe.fit_transform(df[low_cardinality_features])\n",
    "\n",
    "# TargetEncode high cardinality features (assuming the target variable is 'GHGEmissionsIntensity')\n",
    "high_cardinality_features = ['PropertyName', 'Address']\n",
    "encoded_high_cardinality = target_enc.fit_transform(df[high_cardinality_features], df['GHGEmissionsIntensity'])\n",
    "\n",
    "# Dropping original columns and adding encoded ones\n",
    "df.drop(columns=low_cardinality_features + high_cardinality_features, inplace=True)\n",
    "df = pd.concat([df, pd.DataFrame(encoded_low_cardinality), pd.DataFrame(encoded_high_cardinality)], axis=1)\n",
    "```\n",
    "\n",
    "### **3. Create New Variables from Existing Ones (CE3)**:\n",
    "We can create a new variable called `AgeOfBuilding` using `YearBuilt`:\n",
    "```python\n",
    "df['AgeOfBuilding'] = 2023 - df['YearBuilt']  # Assuming current year is 2023\n",
    "```\n",
    "\n",
    "### **4. Mathematical Transformations (CE4)**:\n",
    "If any feature has a skewed distribution, we can use logarithmic transformation or square root transformation. Let's consider `SiteEnergyUse(kBtu)` which seems to be a continuous variable that might be skewed:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Log transform\n",
    "df['Log_SiteEnergyUse'] = np.log1p(df['SiteEnergyUse(kBtu)'])\n",
    "```\n",
    "\n",
    "### **5. Normalize Variables (CE5)**:\n",
    "Variables like `Latitude` and `Longitude` can be normalized using MinMaxScaler or StandardScaler. Here's how you can do it:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_to_normalize = ['Latitude', 'Longitude']\n",
    "\n",
    "df[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n",
    "```\n",
    "\n",
    "With these steps, we've performed feature engineering on the given data. Depending on the model and the use-case, some of these steps might need adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Strategy for Developing the Model (CE1)**:\n",
    "\n",
    "**a. Understand the Business Goal:** \n",
    "The aim is to predict total energy consumption and CO2 consumption based on building characteristics, location, and other features. Such predictions could aid in forecasting energy needs, evaluating the effectiveness of energy-saving measures, or determining carbon footprints.\n",
    "\n",
    "**b. Data Preprocessing:** \n",
    "Ensure data is cleaned, outliers are handled, missing values are dealt with, and categorical variables are transformed.\n",
    "\n",
    "**c. Model Selection:** \n",
    "Start with simpler models and then move to complex models. Compare the performance based on appropriate metrics.\n",
    "\n",
    "**d. Model Evaluation:** \n",
    "Use cross-validation to evaluate model performance, preventing overfitting.\n",
    "\n",
    "**e. Model Deployment (if applicable):** \n",
    "Once the best model is selected, it can be deployed for real-time predictions if needed.\n",
    "\n",
    "### **2. Choose the Relevant Target Variables (CE2)**:\n",
    "- **Total energy consumption**: Based on the given data, it seems `SiteEnergyUse(kBtu)` would be the best representation for total energy consumption.\n",
    "- **CO2 consumption**: This can be inferred from `TotalGHGEmissions`.\n",
    "\n",
    "### **3. Check for Data Leakage (CE3)**:\n",
    "Data leakage can occur when your training data contains information about what you are trying to predict. To prevent this:\n",
    "\n",
    "- Drop features that might be known post-facto but not beforehand. For example, if there's a feature like `PostRetrofitEnergyUsage` which measures energy after a retrofit, it wouldn't be known before and should be removed.\n",
    "- Check correlations between features and target. If some feature has an unusually high correlation with the target, investigate.\n",
    "\n",
    "```python\n",
    "# Checking correlation\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Checking high correlation with the target variables\n",
    "high_corr_with_energy = correlation_matrix[abs(correlation_matrix['SiteEnergyUse(kBtu)']) > 0.9].index\n",
    "high_corr_with_CO2 = correlation_matrix[abs(correlation_matrix['TotalGHGEmissions']) > 0.9].index\n",
    "\n",
    "print(\"Features highly correlated with Energy Use:\", high_corr_with_energy)\n",
    "print(\"Features highly correlated with CO2 Emissions:\", high_corr_with_CO2)\n",
    "```\n",
    "\n",
    "### **4. Test Several Algorithms (CE4)**:\n",
    "**Linear Models**:\n",
    "- Linear Regression\n",
    "- Lasso/Ridge Regression\n",
    "\n",
    "**Non-linear Models**:\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Gradient Boosting Machines\n",
    "\n",
    "Let's use two target variables one by one:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Splitting the data\n",
    "X = df.drop(columns=['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'])\n",
    "y_energy = df['SiteEnergyUse(kBtu)']\n",
    "y_CO2 = df['TotalGHGEmissions']\n",
    "\n",
    "X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(X, y_energy, test_size=0.2, random_state=42)\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X, y_CO2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Model for energy consumption\n",
    "linear_energy = LinearRegression()\n",
    "linear_energy.fit(X_train_e, y_train_e)\n",
    "\n",
    "# Linear Model for CO2 consumption\n",
    "linear_CO2 = LinearRegression()\n",
    "linear_CO2.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Non-linear Model for energy consumption\n",
    "tree_energy = DecisionTreeRegressor()\n",
    "tree_energy.fit(X_train_e, y_train_e)\n",
    "\n",
    "# Non-linear Model for CO2 consumption\n",
    "tree_CO2 = DecisionTreeRegressor()\n",
    "tree_CO2.fit(X_train_c, y_train_c)\n",
    "\n",
    "# You can similarly add more models like Ridge, RandomForest, etc.\n",
    "```\n",
    "\n",
    "Remember to evaluate model performance using suitable metrics (like RMSE) on a validation set or using cross-validation. Then, select the model that performs the best according to your business goal and the evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **CE1: Appropriate Metric**\n",
    "\n",
    "For regression problems like predicting energy consumption or CO2 emissions, RMSE (Root Mean Square Error) or MAE (Mean Absolute Error) are typically used. Since RMSE gives a relatively high weight to large errors, it's a good choice if those are particularly undesirable.\n",
    "\n",
    "### **CE2: Other Performance Indicators**\n",
    "\n",
    "Apart from the main metric (e.g., RMSE), it's crucial to analyze:\n",
    "\n",
    "- **Coefficients**: Especially in linear models, they show the importance and direction of the relationship of each feature to the target.\n",
    "  \n",
    "- **Visualization**: Plot residuals against predicted values. If there's a pattern, the model may not be capturing some aspect of the data.\n",
    "  \n",
    "- **Computation Time**: Can be an important factor if the dataset is large or needs real-time predictions.\n",
    "\n",
    "### **CE3: Separate Train/Test Data**\n",
    "\n",
    "We've already split our data into training and testing sets. It helps in evaluating the model's performance on unseen data and detecting overfitting.\n",
    "\n",
    "### **CE4: Simple Reference Model**\n",
    "\n",
    "Using a `DummyRegressor` will provide a baseline performance which our models should outperform.\n",
    "\n",
    "```python\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "dummy.fit(X_train_e, y_train_e)\n",
    "dummy_rmse = np.sqrt(mean_squared_error(y_test_e, dummy.predict(X_test_e)))\n",
    "```\n",
    "\n",
    "### **CE5 & CE6: Hyper-parameter Optimization and Cross-validation**\n",
    "\n",
    "We can use `GridSearchCV` or `RandomizedSearchCV` to find the best parameters for our model.\n",
    "\n",
    "For simplicity, let's see it for a `RandomForestRegressor`:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), params, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_e, y_train_e)\n",
    "```\n",
    "\n",
    "### **CE7: Presenting Results**\n",
    "\n",
    "After running different models and hyperparameters:\n",
    "\n",
    "1. **Present all models** from simplest (like `DummyRegressor` and `LinearRegression`) to most complex (like `RandomForest` or `GradientBoosting`).\n",
    "   \n",
    "2. **Show RMSE** (or the chosen metric) for each model.\n",
    "\n",
    "3. **Discuss the computation time** for each model, especially if there are significant differences.\n",
    "\n",
    "4. **Final Choice Justification**: It's not always about the lowest RMSE. Sometimes, a slightly worse RMSE might be acceptable if the model is much faster.\n",
    "\n",
    "### **CE8: Feature Importance Analysis**\n",
    "\n",
    "For models that provide feature importances, like `RandomForestRegressor`:\n",
    "\n",
    "1. **Overall Dataset**:\n",
    "\n",
    "```python\n",
    "importances = grid_search.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"{X.columns[indices[i]]}: {importances[indices[i]]}\")\n",
    "```\n",
    "\n",
    "2. **For each individual**: This is a more complex task and requires methods like LIME (Local Interpretable Model-Agnostic Explanations) or SHAP (SHapley Additive exPlanations) to interpret the predictions for individual data points.\n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "By following these steps, you'll have a comprehensive view of model performances and make an informed decision that best aligns with the business objective."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
